{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from random import shuffle, sample\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 1e-3, \n",
    "    'bs': 128, \n",
    "    'epochs': 5, \n",
    "    'num_tasks': 5,\n",
    "    'dataset': \"MNIST\",\n",
    "    'num_classes': 10, \n",
    "    'in_size': 28,\n",
    "    'n_channels': 1,\n",
    "    'hidden_size': 50\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(dataroot, dataset):\n",
    "    if dataset == 'MNIST':\n",
    "        mean, std = (0.1307), (0.3081)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    val_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, tasks_split):\n",
    "    split_dataset = {}\n",
    "    for e, current_classes in tasks_split.items():\n",
    "        task_indices = np.isin(np.array(dataset.targets), current_classes)\n",
    "        split_dataset[e] = Subset(dataset, np.where(task_indices)[0])\n",
    "    return split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(acc):\n",
    "    num_tasks = len(acc)\n",
    "    first_task = list(acc.keys())[0]\n",
    "    sequence_length = len(acc[first_task]) if isinstance(acc[first_task], list) else num_tasks\n",
    "    acc_array = np.zeros((num_tasks, sequence_length))\n",
    "    for task, val in acc.items():\n",
    "        acc_array[int(task), :] = val\n",
    "    return acc_array\n",
    "\n",
    "\n",
    "def plot_accuracy_matrix(array):\n",
    "    num_tasks = array.shape[1]\n",
    "    array = np.round(array, 2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(array, vmin=np.min(array), vmax=np.max(array))\n",
    "    for i in range(len(array)):\n",
    "        for j in range(array.shape[1]):\n",
    "            ax.text(j,i, array[i,j], va='center', ha='center', c='w', fontsize=15)\n",
    "    ax.set_yticks(np.arange(num_tasks))\n",
    "    ax.set_ylabel('Number of tasks')\n",
    "    ax.set_xticks(np.arange(num_tasks))\n",
    "    ax.set_xlabel('Tasks finished')\n",
    "    ax.set_title(f\"ACC: {np.mean(array[:, -1]):.3f} -- std {np.std(np.mean(array[:, -1])):.3f}\")\n",
    "    plt.show()\n",
    "    plt.savefig('matrix')\n",
    "\n",
    "\n",
    "def plot_acc_over_time(array):\n",
    "    fig, ax = plt.subplots()\n",
    "    for e, acc in enumerate(array):\n",
    "        ax.plot(acc, label=e)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('over_time')\n",
    "\n",
    "\n",
    "def compute_average_accuracy(array):\n",
    "    num_tasks = len(array)\n",
    "    avg_acc = np.sum(array[:, -1], axis=0)/num_tasks\n",
    "    return avg_acc\n",
    "\n",
    "\n",
    "def compute_backward_transfer(array):\n",
    "    num_tasks = len(array)\n",
    "    diag = np.diag(array)[:-1] # Note, we do not compute backward transfer for the last task!\n",
    "    end_acc = array[:-1, -1]\n",
    "    bwt = np.sum(end_acc - diag)/(num_tasks - 1)\n",
    "    return bwt\n",
    "\n",
    "\n",
    "def compute_forward_transfer(array, b):\n",
    "    num_tasks = len(array)\n",
    "    sub_diag = np.diag(array, k=-1) # Note, we do not compute forward transfer for the first task!\n",
    "    fwt = np.sum(sub_diag - b[1:])/(num_tasks - 1)\n",
    "    return fwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC -- Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EWC(object):\n",
    "    def __init__(self, model, dataset):\n",
    "\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._means = {}\n",
    "        # Compute a diagonal of empirical Fisher Information Matrix\n",
    "        self._precision_matrices = self._diag_fisher()\n",
    "\n",
    "        # Create a frozen copy of NN-weights to compute the change of weights in penalty method\n",
    "        for n, p in deepcopy(self.params).items():\n",
    "            self._means[n] = p.data\n",
    "\n",
    "    def _diag_fisher(self):\n",
    "        precision_matrices = {}\n",
    "        for n, p in deepcopy(self.params).items():\n",
    "            p.data.zero_()\n",
    "            precision_matrices[n] = p.data\n",
    "\n",
    "        self.model.eval()\n",
    "        for input in self.dataset:\n",
    "            self.model.zero_grad()\n",
    "            input = input.cuda()\n",
    "            output = self.model(input).view(1, -1)\n",
    "            label = output.max(1)[1].view(-1)\n",
    "            loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n",
    "            # We compute the gradients for empirical Fisher; don't use them for the update\n",
    "            loss.backward()\n",
    "\n",
    "            for n, p in self.model.named_parameters():\n",
    "                precision_matrices[n].data += p.grad.data ** 2 / len(self.dataset)\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    \"\"\"\n",
    "      Write the method to compute EWC penalty. \n",
    "      Assume that method takes current model as an input.\n",
    "    \"\"\"\n",
    "    def penalty(self, model):\n",
    "        ##########################\n",
    "        ##### Your code here #####\n",
    "        ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class incremental model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, args, train_datasets, val_datasets):\n",
    "        self.args = args\n",
    "        self.model = MLP(self.args)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr'])\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.reset_acc()\n",
    "        self.train_datasets = train_datasets\n",
    "        self.val_datasets = val_datasets\n",
    "    \n",
    "    def reset_acc(self):\n",
    "        self.acc = {key: [] for key in self.args['task_names']}\n",
    "        self.acc_end = {key: [] for key in self.args['task_names']}\n",
    "\n",
    "    def train(self):\n",
    "        importance = 0.001\n",
    "        sample_size = 1000\n",
    "        for task, data in self.train_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=self.args['bs'], shuffle=True)\n",
    "            print(task)\n",
    "            \"\"\"\n",
    "             Use the access to the train_datasets and create a list of \n",
    "             randomly selected **previous** examples (of length sample_size).\n",
    "             Pass that list with current model to the constructor of EWC class.\n",
    "            \"\"\"\n",
    "            ##########################\n",
    "            ##### Your code here #####\n",
    "            ##########################\n",
    "            for epoch in range(self.args['epochs']):\n",
    "                epoch_loss = 0\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for e, (X, y) in enumerate(loader):\n",
    "                    if torch.cuda.is_available():\n",
    "                        X, y = X.cuda(), y.cuda()\n",
    "                    output = self.model(X)\n",
    "                    \"\"\"\n",
    "                     Modify the loss objective to include the EWC penalty after\n",
    "                     finishing the first task. Use variable importance to scale\n",
    "                     EWC loss.\n",
    "                    \"\"\"\n",
    "                    loss = self.criterion(output, y)\n",
    "                    ##########################\n",
    "                    ##### Your code here #####\n",
    "                    ##########################\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward() \n",
    "                    self.optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y)\n",
    "                    total += len(X)\n",
    "                    if e % 50 == 0:\n",
    "                        self.validate()\n",
    "                print(f\"Epoch {epoch}: Loss {epoch_loss/e:.3f} Acc: {correct/total:.3f}\")\n",
    "            self.validate(end_of_epoch=True)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, end_of_epoch=False):\n",
    "        self.model.eval()\n",
    "        for task, data in self.val_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=args['bs'], shuffle=True)\n",
    "            correct, total = 0, 0\n",
    "            for e, (X, y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    X, y = X.cuda(), y.cuda()\n",
    "                output = self.model(X)\n",
    "                correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y).item()\n",
    "                total += len(X)\n",
    "            self.acc[task].append(correct/total)\n",
    "            if end_of_epoch:\n",
    "                self.acc_end[task].append(correct/total)\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.fc1 = torch.nn.Linear(args['in_size']**2 * args['n_channels'], hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, args['num_classes'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.flatten(start_dim=1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(range(args['num_classes']))\n",
    "shuffle(classes)\n",
    "class_split = {str(i): classes[i*2: (i+1)*2] for i in range(args['num_tasks'])}\n",
    "args['task_names'] = list(class_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
    "train_tasks = split_dataset(train, class_split)\n",
    "val_tasks = split_dataset(test, class_split)\n",
    "agent = Agent(args, train_tasks, val_tasks)\n",
    "\n",
    "agent.validate()\n",
    "random_model_acc = [i[0] for i in agent.acc.values()]\n",
    "agent.reset_acc()\n",
    "agent.train()\n",
    "\n",
    "acc_at_end_arr = dict2array(agent.acc_end)\n",
    "plot_accuracy_matrix(acc_at_end_arr)\n",
    "\n",
    "acc_arr = dict2array(agent.acc)\n",
    "plot_acc_over_time(acc_arr)\n",
    "\n",
    "print(f\"The average accuracy at the end of sequence is: {compute_average_accuracy(acc_at_end_arr):.3f}\")\n",
    "print(f\"BWT:'{compute_backward_transfer(acc_at_end_arr):.3f}'\")\n",
    "print(f\"FWT:'{compute_forward_transfer(acc_at_end_arr, random_model_acc):.3f}'\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
