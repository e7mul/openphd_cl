{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from random import shuffle\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"GPU device not found, selection Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define basic hyperparameters for the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 1e-3, \n",
    "    'bs': 128, \n",
    "    'epochs': 2, \n",
    "    'num_tasks': 5,\n",
    "    'dataset': \"MNIST\",\n",
    "    'num_classes': 10, \n",
    "    'in_size': 28,\n",
    "    'n_channels': 1,\n",
    "    'hidden_size': 50\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sequence of tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataroot, dataset):\n",
    "    if dataset == 'MNIST':\n",
    "        mean, std = (0.1307), (0.3081)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataset,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    val_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataset,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, tasks_split):\n",
    "    split_dataset = {}\n",
    "    for e, current_classes in tasks_split.items():\n",
    "        task_indices = np.isin(np.array(dataset.targets), current_classes)\n",
    "        split_dataset[e] = Subset(dataset, np.where(task_indices)[0])\n",
    "    return split_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Incremental example\n",
    "\n",
    "Papers about continual learning scenarios: https://arxiv.org/abs/1904.07734 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIAgent:\n",
    "    def __init__(self, args, train_datasets, val_datasets):\n",
    "        self.args = args\n",
    "        self.model = MLP(self.args)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr'])\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.reset_acc()\n",
    "        self.train_datasets = train_datasets\n",
    "        self.val_datasets = val_datasets\n",
    "    \n",
    "    def reset_acc(self):\n",
    "        self.acc = {key: [] for key in self.args['task_names']}\n",
    "        self.acc_end = {key: [] for key in self.args['task_names']}\n",
    "\n",
    "    def train(self):\n",
    "        for task, data in self.train_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=self.args['bs'], shuffle=True)\n",
    "            for epoch in range(self.args['epochs']):\n",
    "                epoch_loss = 0\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for e, (X, y) in enumerate(loader):\n",
    "                    if torch.cuda.is_available():\n",
    "                        X, y = X.cuda(), y.cuda()\n",
    "                    output = self.model(X)\n",
    "                    loss = self.criterion(output, y)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward() \n",
    "                    self.optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y)\n",
    "                    total += len(X)\n",
    "                    if e % 50 == 0:\n",
    "                        self.validate()\n",
    "                print(f\"Epoch {epoch}: Loss {epoch_loss/e:.3f} Acc: {correct/total:.3f}\")\n",
    "            self.validate(end_of_epoch=True)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, end_of_epoch=False):\n",
    "        self.model.eval()\n",
    "        for task, data in self.val_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=args['bs'], shuffle=True)\n",
    "            correct, total = 0, 0\n",
    "            for e, (X, y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    X, y = X.cuda(), y.cuda()\n",
    "                output = self.model(X)\n",
    "                correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y).item()\n",
    "                total += len(X)\n",
    "            self.acc[task].append(correct/total)\n",
    "            if end_of_epoch:\n",
    "                self.acc_end[task].append(correct/total)\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.fc1 = torch.nn.Linear(args['in_size']**2 * args['n_channels'], hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, args['num_classes'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.flatten(start_dim=1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1\n",
    "\n",
    "Using the definition and examples of other metrics provided below, implement the function \n",
    "which computes the forward transfer. \n",
    "\n",
    "You can assume that array is a square matrix of size TxT, where T is the number of tasks. \n",
    "\n",
    "b is a list of length T, each entry in b is the accuracy of randomly initialized model on respective task.\n",
    "\n",
    "**Pay attention that our array is transposed with respect to the one provided definitions**\n",
    "\n",
    "Papers about CL metrics: https://arxiv.org/abs/1706.08840, https://arxiv.org/abs/1810.13166  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_accuracy(array):\n",
    "    num_tasks = len(array)\n",
    "    avg_acc = np.sum(array[:, -1], axis=0)/num_tasks\n",
    "    return avg_acc\n",
    "\n",
    "\n",
    "def compute_backward_transfer(array):\n",
    "    num_tasks = len(array)\n",
    "    diag = np.diag(array)[:-1] # Note, we do not compute backward transfer for the last task!\n",
    "    end_acc = array[:-1, -1]\n",
    "    bwt = np.sum(end_acc - diag)/(num_tasks - 1)\n",
    "    return bwt\n",
    "\n",
    "\n",
    "def compute_forward_transfer(array, b):\n",
    "    num_tasks = len(array)\n",
    "    sub_diag = np.diag(array, k=-1) # Note, we do not compute forward transfer for the first task!\n",
    "    fwt = np.sum(sub_diag - b[1:])/(num_tasks - 1)\n",
    "    return fwt\n",
    "\n",
    "\n",
    "def dict2array(acc):\n",
    "    num_tasks = len(acc)\n",
    "    first_task = list(acc.keys())[0]\n",
    "    sequence_length = len(acc[first_task]) if isinstance(acc[first_task], list) else num_tasks\n",
    "    acc_array = np.zeros((num_tasks, sequence_length))\n",
    "    for task, val in acc.items():\n",
    "        acc_array[int(task), :] = val\n",
    "    return acc_array\n",
    "\n",
    "\n",
    "def plot_accuracy_matrix(array):\n",
    "    num_tasks = array.shape[1]\n",
    "    array = np.round(array, 2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(array, vmin=np.min(array), vmax=np.max(array))\n",
    "    for i in range(len(array)):\n",
    "        for j in range(array.shape[1]):\n",
    "            ax.text(j,i, array[i,j], va='center', ha='center', c='w', fontsize=15)\n",
    "    ax.set_yticks(np.arange(num_tasks))\n",
    "    ax.set_ylabel('Number of tasks')\n",
    "    ax.set_xticks(np.arange(num_tasks))\n",
    "    ax.set_xlabel('Tasks finished')\n",
    "    ax.set_title(f\"ACC: {np.mean(array[:, -1]):.3f} -- std {np.std(np.mean(array[:, -1])):.3f}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc_over_time(array):\n",
    "    fig, ax = plt.subplots()\n",
    "    for e, acc in enumerate(array):\n",
    "        ax.plot(acc, label=e)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Incremental Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(range(args['num_classes']))\n",
    "shuffle(classes)\n",
    "class_split = {str(i): classes[i*2: (i+1)*2] for i in range(args['num_tasks'])}\n",
    "print(class_split)\n",
    "args['task_names'] = list(class_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the datasets according to sampled split\n",
    "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
    "train_tasks = split_dataset(train, class_split)\n",
    "val_tasks = split_dataset(test, class_split)\n",
    "# Create the agent & initialize the network\n",
    "agent = CIAgent(args, train_tasks, val_tasks)\n",
    "# Check & save (for the FWT metric) the accuracy of randomly initialized model\n",
    "agent.validate()\n",
    "random_model_acc = [i[0] for i in agent.acc.values()]\n",
    "agent.reset_acc()\n",
    "\n",
    "# Train the agent on the whole sequence of tasks\n",
    "agent.train()\n",
    "\n",
    "# Get accuracy of the agent at the end of each task\n",
    "acc_at_end_arr = dict2array(agent.acc_end)\n",
    "plot_accuracy_matrix(acc_at_end_arr)\n",
    "\n",
    "\n",
    "# Get intermediate accuracy\n",
    "acc_arr = dict2array(agent.acc)\n",
    "plot_acc_over_time(acc_arr)\n",
    "\n",
    "print(f\"The average accuracy at the end of sequence is: {compute_average_accuracy(acc_at_end_arr):.3f}\")\n",
    "print(f\"BWT:'{compute_backward_transfer(acc_at_end_arr):.3f}'\")\n",
    "print(f\"FWT:'{compute_forward_transfer(acc_at_end_arr, random_model_acc):.3f}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2\n",
    "\n",
    "Modify classes MLP and Agent in order to create a Task Incremental scenario.\n",
    "\n",
    "The network should consists of 5 heads (one head for each task).\n",
    "\n",
    "To create separate heads use the torch.nn.ModuleDict method.\n",
    "\n",
    "\n",
    "**Pay attention to the remapping of labels.** For each task, the labels starts from 0.\n",
    "\n",
    "You should also modify the train and validation methods so that the network output\n",
    "is conditioned on the currently processed task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RemapClasses(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, class_list):\n",
    "        self.dataset = dataset\n",
    "        self.class_list = {target.item(): e for e, target in enumerate(torch.unique(class_list))}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "        return img, self.class_list[target]\n",
    "\n",
    "\n",
    "def split_dataset(dataset, tasks_split):\n",
    "    split_dataset = {}\n",
    "    for e, current_classes in tasks_split.items():\n",
    "        task_indices = np.isin(np.array(dataset.targets), current_classes)\n",
    "        split_dataset[e] = RemapClasses(Subset(dataset, np.where(task_indices)[0]), torch.tensor(dataset.targets)[task_indices])\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "class TIAgent:\n",
    "    def __init__(self, args, train_datasets, val_datasets):\n",
    "        self.args = args\n",
    "        self.model = MLP(self.args)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr'])\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.reset_acc()\n",
    "        self.train_datasets = train_datasets\n",
    "        self.val_datasets = val_datasets\n",
    "\n",
    "    def reset_acc(self):\n",
    "        self.acc = {key: [] for key in self.args['task_names']}\n",
    "        self.acc_end = {key: [] for key in self.args['task_names']}\n",
    "\n",
    "    def train(self):\n",
    "        for task, data in self.train_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=self.args['bs'], shuffle=True)\n",
    "            for epoch in range(self.args['epochs']):\n",
    "                epoch_loss = 0\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for e, (X, y) in enumerate(loader):\n",
    "                    if torch.cuda.is_available():\n",
    "                        X, y = X.cuda(), y.cuda()\n",
    "                    output = self.model(X, task)\n",
    "                    loss = self.criterion(output, y)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward() \n",
    "                    self.optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y)\n",
    "                    total += len(X)\n",
    "                    if e % 50 == 0:\n",
    "                        self.validate()\n",
    "                print(f\"Epoch {epoch}: Loss {epoch_loss/e:.3f} Acc: {correct/total:.3f}\")\n",
    "            self.validate(end_of_epoch=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, end_of_epoch=False):\n",
    "        self.model.eval()\n",
    "        for task, data in self.val_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=args['bs'], shuffle=True)\n",
    "            correct, total = 0, 0\n",
    "            for e, (X, y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    X, y = X.cuda(), y.cuda()\n",
    "                output = self.model(X, task)\n",
    "                correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y).item()\n",
    "                total += len(X)\n",
    "            self.acc[task].append(round(correct/total, 2))\n",
    "            if end_of_epoch:\n",
    "                self.acc_end[task].append(correct/total)\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.fc1 = torch.nn.Linear(args['in_size']**2 * args['n_channels'], hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = torch.nn.ModuleDict({str(task): torch.nn.Linear(hidden_size, args['num_classes']//args['num_tasks']) for task in range(args[\"num_tasks\"])})\n",
    "\n",
    "    def forward(self, input, task):\n",
    "        x = input.flatten(start_dim=1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4[task](x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Incremental Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(range(args['num_classes']))\n",
    "shuffle(classes)\n",
    "class_split = {str(i): classes[i*2: (i+1)*2] for i in range(args['num_tasks'])}\n",
    "print(class_split)\n",
    "args['task_names'] = list(class_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the datasets according to sampled split\n",
    "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
    "train_tasks = split_dataset(train, class_split)\n",
    "val_tasks = split_dataset(test, class_split)\n",
    "# Create the agent & initialize the network\n",
    "agent = TIAgent(args, train_tasks, val_tasks)\n",
    "# Check & save (for the FWT metric) the accuracy of randomly initialized model\n",
    "agent.validate()\n",
    "random_model_acc = [i[0] for i in agent.acc.values()]\n",
    "agent.reset_acc()\n",
    "\n",
    "# Train the agent on the whole sequence of tasks\n",
    "agent.train()\n",
    "\n",
    "# Get accuracy of the agent at the end of each task\n",
    "acc_at_end_arr = dict2array(agent.acc_end)\n",
    "plot_accuracy_matrix(acc_at_end_arr)\n",
    "\n",
    "\n",
    "# Get intermediate accuracy\n",
    "acc_arr = dict2array(agent.acc)\n",
    "plot_acc_over_time(acc_arr)\n",
    "\n",
    "print(f\"The average accuracy at the end of sequence is: {compute_average_accuracy(acc_at_end_arr):.3f}\")\n",
    "print(f\"BWT:'{compute_backward_transfer(acc_at_end_arr):.3f}'\")\n",
    "print(f\"FWT:'{compute_forward_transfer(acc_at_end_arr, random_model_acc):.3f}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
